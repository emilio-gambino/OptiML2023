{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GybIC15XZMS",
        "outputId": "f6b2b3de-30ab-4fe1-aa6a-daf211e9967f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orbIdxhW7gsi",
        "outputId": "729cb930-c67e-45f0-b472-1b8b84ef3190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13301791.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "### LOADING DATA ###\n",
        "\n",
        "def get_train_valid_loader(data_dir, batch_size, augment, random_seed, valid_size=0.1, shuffle=True):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "\n",
        "    # Define transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((227, 227)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    # Load the dataset\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx1, train_idx2 = indices[split:(split + num_train) // 2], indices[(split + num_train) // 2:]\n",
        "    valid_idx = indices[:split]\n",
        "    train_sampler1 = SubsetRandomSampler(train_idx1)\n",
        "    train_sampler2 = SubsetRandomSampler(train_idx2)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader1 = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler1\n",
        "    )\n",
        "    train_loader2 = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler2\n",
        "    )\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler\n",
        "    )\n",
        "\n",
        "    return (train_loader1, train_loader2, valid_loader)\n",
        "\n",
        "\n",
        "def get_test_loader(data_dir, batch_size, shuffle=True):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    )\n",
        "    # define transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((227,227)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=False,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle\n",
        "    )\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "# CIFAR10 dataset\n",
        "train_loader1, train_loader2, valid_loader = get_train_valid_loader(data_dir = './data', batch_size = 64, augment = False, random_seed = 1)\n",
        "\n",
        "test_loader = get_test_loader(data_dir = './data', batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Boq1VmyiYECp"
      },
      "outputs": [],
      "source": [
        "## taken from https://blog.paperspace.com/alexnet-pytorch/\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(9216, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pu_3qkzfRlhE"
      },
      "outputs": [],
      "source": [
        "### QUANTIZATION CODE ###\n",
        "\n",
        "def quantize(vec, num_bits, stochastic):\n",
        "    \"\"\"\n",
        "    :param : torch tensor, number of bits\n",
        "    :return: quantized vector\n",
        "    \"\"\"\n",
        "    s = 2 ** num_bits\n",
        "    shape = vec.shape\n",
        "\n",
        "    # Normalized vector\n",
        "    vec = vec.view(-1)\n",
        "    # norm = torch.norm(vec, dim=1, keepdim=True)\n",
        "    norm = torch.max(torch.abs(vec), dim=0, keepdim=True)[0]\n",
        "    normalized_vec = vec / norm\n",
        "\n",
        "    # Epsilon\n",
        "    scaled_vec = (torch.abs(normalized_vec) * s).to(device)\n",
        "    l = torch.clamp(scaled_vec, 0, s-1).type(torch.int32)\n",
        "\n",
        "    if stochastic:\n",
        "      probabilities = scaled_vec - l.type(torch.float32)\n",
        "      r = torch.rand(l.size()).to(device)\n",
        "      l[:] += (probabilities > r).type(torch.int32) - (l == s-1).type(torch.int32)\n",
        "\n",
        "    # Sign\n",
        "    signs = torch.where(torch.sign(vec) > 0, torch.tensor(1), torch.tensor(-1)).to(device)\n",
        "\n",
        "    m = torch.mul(torch.mul(l.view(shape), signs.view(shape)), norm / (s-1.0))\n",
        "    return m\n",
        "\n",
        "def map_gradients(model, num_bits, stochastic, previous):\n",
        "  quantized_grads = []\n",
        "  prev_grads = []\n",
        "  for p, prev in zip(model.parameters(), previous):\n",
        "    gradients = quantize(p.grad.data.clone() + prev, num_bits, stochastic) # compute quantized gradient\n",
        "    p.grad.data.copy_(gradients) # set gradient to quantized gradient\n",
        "    quantized_grads.append(gradients)\n",
        "    prev_grads.append(p.grad.data - gradients)\n",
        "  return quantized_grads, prev_grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xsCBtyNeJfWC"
      },
      "outputs": [],
      "source": [
        "def test_and_save_best(mem_models, num_bits):\n",
        "  # Sort the models based on accuracy in descending order\n",
        "  sorted_models = sorted(mem_models, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "  # Select the top 3 models\n",
        "  top_models = sorted_models[:3]\n",
        "\n",
        "  for acc, model, epoch in top_models:\n",
        "    print(\"Epoch : \" + str(epoch) + \", Validation Accuracy : \" + str(acc))\n",
        "\n",
        "  best_accuracy = 0.0\n",
        "  best_model = None\n",
        "  best_epoch = 0\n",
        "\n",
        "  # Evaluate the top models on the test data\n",
        "  for accuracy, model, epoch in top_models:\n",
        "      model.eval()  # Set the model to evaluation mode\n",
        "      with torch.no_grad():\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          for images, labels in test_loader:\n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "              outputs = model(images)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "          accuracy = 100 * correct / total\n",
        "          print('Accuracy of the network on the {} test images: {} %'.format(10000, accuracy))\n",
        "\n",
        "          # Check if this model has the best accuracy so far\n",
        "          if accuracy > best_accuracy:\n",
        "              best_accuracy = accuracy\n",
        "              best_model = model\n",
        "              best_epoch = epoch\n",
        "\n",
        "  # Save the best model to disk\n",
        "  torch.save(best_model.state_dict(), '/content/drive/MyDrive/OptML/best_model_{}_{}_{}.pth'.format(num_bits, best_accuracy, best_epoch))\n",
        "  print('Best model saved with accuracy: {} %, at epoch : {}'.format(best_accuracy, best_epoch))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### BASELINE MODEL ###\n",
        "\n",
        "def get_train_valid_loader(data_dir,batch_size,augment,random_seed,valid_size=0.1,shuffle=True):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "\n",
        "    # define transforms\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize((227,227)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "    ])\n",
        "\n",
        "    # load the dataset\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "\n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "\n",
        "def get_test_loader(data_dir,batch_size,shuffle=True):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    )\n",
        "\n",
        "    # define transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((227,227)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=False,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "# CIFAR10 dataset\n",
        "train_loader, valid_loader = get_train_valid_loader(data_dir = './data',batch_size = 64,\n",
        "                       augment = False,random_seed = 1)\n",
        "\n",
        "test_loader = get_test_loader(data_dir = './data',\n",
        "                              batch_size = 64)\n",
        "\n",
        "num_classes = 10\n",
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "mem_models = []\n",
        "\n",
        "model = AlexNet(num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
        "        if epoch > 8:\n",
        "          mem_models += [(correct / total, copy.deepcopy(model), epoch+1)]\n",
        "\n",
        "test_and_save_best(mem_models, 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U81fA0sKHOXE",
        "outputId": "aee817c9-43e1-45a9-9c33-01b431f9bdee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/20], Step [704/704], Loss: 1.5231\n",
            "Accuracy of the network on the 5000 validation images: 61.1 %\n",
            "Epoch [2/20], Step [704/704], Loss: 1.6057\n",
            "Accuracy of the network on the 5000 validation images: 67.9 %\n",
            "Epoch [3/20], Step [704/704], Loss: 0.6121\n",
            "Accuracy of the network on the 5000 validation images: 72.14 %\n",
            "Epoch [4/20], Step [704/704], Loss: 0.5621\n",
            "Accuracy of the network on the 5000 validation images: 76.22 %\n",
            "Epoch [5/20], Step [704/704], Loss: 0.2092\n",
            "Accuracy of the network on the 5000 validation images: 76.68 %\n",
            "Epoch [6/20], Step [704/704], Loss: 1.0236\n",
            "Accuracy of the network on the 5000 validation images: 75.66 %\n",
            "Epoch [7/20], Step [704/704], Loss: 1.1628\n",
            "Accuracy of the network on the 5000 validation images: 77.14 %\n",
            "Epoch [8/20], Step [704/704], Loss: 0.2319\n",
            "Accuracy of the network on the 5000 validation images: 79.24 %\n",
            "Epoch [9/20], Step [704/704], Loss: 0.0367\n",
            "Accuracy of the network on the 5000 validation images: 81.1 %\n",
            "Epoch [10/20], Step [704/704], Loss: 0.4295\n",
            "Accuracy of the network on the 5000 validation images: 81.0 %\n",
            "Epoch [11/20], Step [704/704], Loss: 0.7468\n",
            "Accuracy of the network on the 5000 validation images: 80.98 %\n",
            "Epoch [12/20], Step [704/704], Loss: 0.4379\n",
            "Accuracy of the network on the 5000 validation images: 78.5 %\n",
            "Epoch [13/20], Step [704/704], Loss: 0.2909\n",
            "Accuracy of the network on the 5000 validation images: 79.94 %\n",
            "Epoch [14/20], Step [704/704], Loss: 0.5815\n",
            "Accuracy of the network on the 5000 validation images: 82.5 %\n",
            "Epoch [15/20], Step [704/704], Loss: 0.4742\n",
            "Accuracy of the network on the 5000 validation images: 81.72 %\n",
            "Epoch [16/20], Step [704/704], Loss: 0.4926\n",
            "Accuracy of the network on the 5000 validation images: 82.6 %\n",
            "Epoch [17/20], Step [704/704], Loss: 1.4732\n",
            "Accuracy of the network on the 5000 validation images: 81.26 %\n",
            "Epoch [18/20], Step [704/704], Loss: 0.4021\n",
            "Accuracy of the network on the 5000 validation images: 81.8 %\n",
            "Epoch [19/20], Step [704/704], Loss: 0.0615\n",
            "Accuracy of the network on the 5000 validation images: 82.98 %\n",
            "Epoch [20/20], Step [704/704], Loss: 1.4056\n",
            "Accuracy of the network on the 5000 validation images: 82.1 %\n",
            "Epoch : 19, Validation Accuracy : 0.8298\n",
            "Epoch : 16, Validation Accuracy : 0.826\n",
            "Epoch : 14, Validation Accuracy : 0.825\n",
            "Accuracy of the network on the 10000 test images: 82.45 %\n",
            "Accuracy of the network on the 10000 test images: 83.25 %\n",
            "Accuracy of the network on the 10000 test images: 83.04 %\n",
            "Best model saved with accuracy: 83.25 %, at epoch : 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6e0VMuSCb9r",
        "outputId": "141236b3-e557-4913-ac5b-67441bdced39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Step [352/352], Loss: 1.2077\n",
            "Accuracy of the network on the 5000 validation images: 57.08 %\n",
            "Epoch [2/25], Step [352/352], Loss: 1.1147\n",
            "Accuracy of the network on the 5000 validation images: 68.56 %\n",
            "Epoch [3/25], Step [352/352], Loss: 0.8194\n",
            "Accuracy of the network on the 5000 validation images: 70.88 %\n",
            "Epoch [4/25], Step [352/352], Loss: 0.5518\n",
            "Accuracy of the network on the 5000 validation images: 74.96 %\n",
            "Epoch [5/25], Step [352/352], Loss: 0.4984\n",
            "Accuracy of the network on the 5000 validation images: 77.14 %\n",
            "Epoch [6/25], Step [352/352], Loss: 0.5769\n",
            "Accuracy of the network on the 5000 validation images: 78.52 %\n",
            "Epoch [7/25], Step [352/352], Loss: 0.4584\n",
            "Accuracy of the network on the 5000 validation images: 79.36 %\n",
            "Epoch [8/25], Step [352/352], Loss: 0.4030\n",
            "Accuracy of the network on the 5000 validation images: 78.72 %\n",
            "Epoch [9/25], Step [352/352], Loss: 0.4573\n",
            "Accuracy of the network on the 5000 validation images: 80.84 %\n",
            "Epoch [10/25], Step [352/352], Loss: 0.6513\n",
            "Accuracy of the network on the 5000 validation images: 81.0 %\n",
            "Epoch [11/25], Step [352/352], Loss: 0.5041\n",
            "Accuracy of the network on the 5000 validation images: 82.22 %\n",
            "Epoch [12/25], Step [352/352], Loss: 0.4500\n",
            "Accuracy of the network on the 5000 validation images: 81.38 %\n",
            "Epoch [13/25], Step [352/352], Loss: 0.5298\n",
            "Accuracy of the network on the 5000 validation images: 82.2 %\n",
            "Epoch [14/25], Step [352/352], Loss: 0.5849\n",
            "Accuracy of the network on the 5000 validation images: 82.2 %\n",
            "Epoch [15/25], Step [352/352], Loss: 0.4247\n",
            "Accuracy of the network on the 5000 validation images: 82.1 %\n",
            "Epoch [16/25], Step [352/352], Loss: 0.2899\n",
            "Accuracy of the network on the 5000 validation images: 83.26 %\n",
            "Epoch [17/25], Step [352/352], Loss: 0.1196\n",
            "Accuracy of the network on the 5000 validation images: 83.56 %\n",
            "Epoch [18/25], Step [352/352], Loss: 0.2567\n",
            "Accuracy of the network on the 5000 validation images: 83.98 %\n",
            "Epoch [19/25], Step [352/352], Loss: 0.0706\n",
            "Accuracy of the network on the 5000 validation images: 83.82 %\n",
            "Epoch [20/25], Step [352/352], Loss: 0.1548\n",
            "Accuracy of the network on the 5000 validation images: 83.7 %\n",
            "Epoch [21/25], Step [352/352], Loss: 0.2762\n",
            "Accuracy of the network on the 5000 validation images: 83.04 %\n",
            "Epoch [22/25], Step [352/352], Loss: 0.1884\n",
            "Accuracy of the network on the 5000 validation images: 82.86 %\n",
            "Epoch [23/25], Step [352/352], Loss: 0.2702\n",
            "Accuracy of the network on the 5000 validation images: 83.26 %\n",
            "Epoch [24/25], Step [352/352], Loss: 0.5176\n",
            "Accuracy of the network on the 5000 validation images: 84.26 %\n",
            "Epoch [25/25], Step [352/352], Loss: 0.1607\n",
            "Accuracy of the network on the 5000 validation images: 83.66 %\n",
            "Epoch : 24, Validation Accuracy : 0.8426\n",
            "Epoch : 18, Validation Accuracy : 0.8398\n",
            "Epoch : 19, Validation Accuracy : 0.8382\n",
            "Accuracy of the network on the 10000 test images: 83.87 %\n",
            "Accuracy of the network on the 10000 test images: 84.2 %\n",
            "Accuracy of the network on the 10000 test images: 84.11 %\n",
            "Best model saved with accuracy: 84.2 %, at epoch : 18\n"
          ]
        }
      ],
      "source": [
        "##### 2 NODES, 8 BIT QUANT, ERROR CORRECTION, STOCHASTIC ######  3h50min, 84.2%\n",
        "\n",
        "num_classes = 10\n",
        "num_epochs = 25\n",
        "learning_rate = 0.005\n",
        "nodes = 2\n",
        "\n",
        "num_bits = 8\n",
        "stochastic = True\n",
        "\n",
        "mem_models = []\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = AlexNet(num_classes).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "total_step = len(train_loader1)\n",
        "\n",
        "prev1 = [torch.zeros(p.shape).to(device) for p in model.parameters()]\n",
        "prev2 = [torch.zeros(p.shape).to(device) for p in model.parameters()]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (samples1, samples2) in enumerate(zip(train_loader1, train_loader2)):\n",
        "\n",
        "        # Separate data per node\n",
        "        images1, labels1 = samples1[0].to(device), samples1[1].to(device)\n",
        "        images2, labels2 = samples2[0].to(device), samples2[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # First forward and backward pass\n",
        "        outs = model(images1)\n",
        "        loss1 = criterion(outs, labels1)\n",
        "        loss1.backward()\n",
        "        grad1, prev1 = map_gradients(model, num_bits, stochastic, prev1)\n",
        "\n",
        "        # Clear gradients from first pass (not necessary)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Second forward and backward pass\n",
        "        outs = model(images2)\n",
        "        loss2 = criterion(outs, labels2)\n",
        "        loss2.backward()\n",
        "        _, prev2 = map_gradients(model, num_bits, stochastic, prev2)\n",
        "\n",
        "        # Average the quantized gradients\n",
        "        for p, g in zip(model.parameters(), grad1):\n",
        "          p.grad.data.copy_(torch.add(p.grad.data, g))\n",
        "          p.grad.data.mul_(1.0 / nodes)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss2.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
        "\n",
        "        if epoch > 8:\n",
        "          mem_models += [(correct / total, copy.deepcopy(model), epoch+1)]\n",
        "\n",
        "test_and_save_best(mem_models, num_bits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvLILrLVlR55",
        "outputId": "49419b0e-d860-4168-c8e7-3a737508d3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Step [352/352], Loss: 1.5242\n",
            "Accuracy of the network on the 5000 validation images: 57.58 %\n",
            "Epoch [2/25], Step [352/352], Loss: 0.7497\n",
            "Accuracy of the network on the 5000 validation images: 69.14 %\n",
            "Epoch [3/25], Step [352/352], Loss: 0.6480\n",
            "Accuracy of the network on the 5000 validation images: 73.86 %\n",
            "Epoch [4/25], Step [352/352], Loss: 0.6750\n",
            "Accuracy of the network on the 5000 validation images: 75.86 %\n",
            "Epoch [5/25], Step [352/352], Loss: 0.6826\n",
            "Accuracy of the network on the 5000 validation images: 76.16 %\n",
            "Epoch [6/25], Step [352/352], Loss: 0.5798\n",
            "Accuracy of the network on the 5000 validation images: 77.02 %\n",
            "Epoch [7/25], Step [352/352], Loss: 0.8281\n",
            "Accuracy of the network on the 5000 validation images: 78.34 %\n",
            "Epoch [8/25], Step [352/352], Loss: 0.2373\n",
            "Accuracy of the network on the 5000 validation images: 78.66 %\n",
            "Epoch [9/25], Step [352/352], Loss: 0.5098\n",
            "Accuracy of the network on the 5000 validation images: 79.56 %\n",
            "Epoch [10/25], Step [352/352], Loss: 0.5335\n",
            "Accuracy of the network on the 5000 validation images: 81.04 %\n",
            "Epoch [11/25], Step [352/352], Loss: 0.4018\n",
            "Accuracy of the network on the 5000 validation images: 80.88 %\n",
            "Epoch [12/25], Step [352/352], Loss: 0.3763\n",
            "Accuracy of the network on the 5000 validation images: 81.6 %\n",
            "Epoch [13/25], Step [352/352], Loss: 0.3514\n",
            "Accuracy of the network on the 5000 validation images: 82.32 %\n",
            "Epoch [14/25], Step [352/352], Loss: 0.3443\n",
            "Accuracy of the network on the 5000 validation images: 81.62 %\n",
            "Epoch [15/25], Step [352/352], Loss: 0.3785\n",
            "Accuracy of the network on the 5000 validation images: 83.3 %\n",
            "Epoch [16/25], Step [352/352], Loss: 0.1288\n",
            "Accuracy of the network on the 5000 validation images: 83.04 %\n",
            "Epoch [17/25], Step [352/352], Loss: 0.3679\n",
            "Accuracy of the network on the 5000 validation images: 82.94 %\n",
            "Epoch [18/25], Step [352/352], Loss: 0.2437\n",
            "Accuracy of the network on the 5000 validation images: 84.26 %\n",
            "Epoch [19/25], Step [352/352], Loss: 0.1090\n",
            "Accuracy of the network on the 5000 validation images: 84.26 %\n",
            "Epoch [20/25], Step [352/352], Loss: 0.3003\n",
            "Accuracy of the network on the 5000 validation images: 83.06 %\n",
            "Epoch [21/25], Step [352/352], Loss: 0.1177\n",
            "Accuracy of the network on the 5000 validation images: 83.94 %\n",
            "Epoch [22/25], Step [352/352], Loss: 0.1815\n",
            "Accuracy of the network on the 5000 validation images: 84.2 %\n",
            "Epoch [23/25], Step [352/352], Loss: 0.2546\n",
            "Accuracy of the network on the 5000 validation images: 83.8 %\n",
            "Epoch [24/25], Step [352/352], Loss: 0.2633\n",
            "Accuracy of the network on the 5000 validation images: 84.18 %\n",
            "Epoch [25/25], Step [352/352], Loss: 0.1899\n",
            "Accuracy of the network on the 5000 validation images: 85.3 %\n",
            "Epoch : 25, Validation Accuracy : 0.853\n",
            "Epoch : 18, Validation Accuracy : 0.8426\n",
            "Epoch : 19, Validation Accuracy : 0.8426\n",
            "Accuracy of the network on the 10000 test images: 84.53 %\n",
            "Accuracy of the network on the 10000 test images: 84.09 %\n",
            "Accuracy of the network on the 10000 test images: 84.65 %\n",
            "Best model saved with accuracy: 84.65 %, at epoch : 19\n"
          ]
        }
      ],
      "source": [
        "##### 2 NODES, 4 BIT QUANT, ERROR CORRECTION, STOCHASTIC ######  4h9min, 85.19%\n",
        "\n",
        "num_classes = 10\n",
        "num_epochs = 25\n",
        "learning_rate = 0.005\n",
        "nodes = 2\n",
        "\n",
        "num_bits = 4\n",
        "stochastic = True\n",
        "\n",
        "mem_models = []\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = AlexNet(num_classes).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "total_step = len(train_loader1)\n",
        "\n",
        "prev1 = [torch.zeros(p.shape).to(device) for p in model.parameters()]\n",
        "prev2 = [torch.zeros(p.shape).to(device) for p in model.parameters()]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (samples1, samples2) in enumerate(zip(train_loader1, train_loader2)):\n",
        "\n",
        "        # Separate data per node\n",
        "        images1, labels1 = samples1[0].to(device), samples1[1].to(device)\n",
        "        images2, labels2 = samples2[0].to(device), samples2[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # First forward and backward pass\n",
        "        outs = model(images1)\n",
        "        loss1 = criterion(outs, labels1)\n",
        "        loss1.backward()\n",
        "        grad1, prev1 = map_gradients(model, num_bits, stochastic, prev1)\n",
        "\n",
        "        # Clear gradients from first pass (not necessary)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Second forward and backward pass\n",
        "        outs = model(images2)\n",
        "        loss2 = criterion(outs, labels2)\n",
        "        loss2.backward()\n",
        "        _, prev2 = map_gradients(model, num_bits, stochastic, prev2)\n",
        "\n",
        "        # Average the quantized gradients\n",
        "        for p, g in zip(model.parameters(), grad1):\n",
        "          p.grad.data.copy_(torch.add(p.grad.data, g))\n",
        "          p.grad.data.mul_(1.0 / nodes)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss2.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
        "\n",
        "        if epoch > 8:\n",
        "          mem_models += [(correct / total, copy.deepcopy(model), epoch+1)]\n",
        "\n",
        "test_and_save_best(mem_models, num_bits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_and_save_best(mem_models, num_bits) # Took 4th best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95JCTWaPFbKt",
        "outputId": "03bc3cb8-28c1-4e00-c460-807472d837a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 25, Validation Accuracy : 0.853\n",
            "Epoch : 18, Validation Accuracy : 0.8426\n",
            "Epoch : 19, Validation Accuracy : 0.8426\n",
            "Epoch : 22, Validation Accuracy : 0.842\n",
            "Accuracy of the network on the 10000 test images: 84.53 %\n",
            "Accuracy of the network on the 10000 test images: 84.09 %\n",
            "Accuracy of the network on the 10000 test images: 84.65 %\n",
            "Accuracy of the network on the 10000 test images: 85.19 %\n",
            "Best model saved with accuracy: 85.19 %, at epoch : 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### 2 NODES, 2 BIT QUANT, ERROR CORRECTION, STOCHASTIC ######  4h37min, 84.3%\n",
        "\n",
        "num_classes = 10\n",
        "num_epochs = 30\n",
        "learning_rate = 0.005\n",
        "nodes = 2\n",
        "\n",
        "num_bits = 2\n",
        "stochastic = True\n",
        "\n",
        "mem_models = []\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = AlexNet(num_classes).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "total_step = len(train_loader1)\n",
        "\n",
        "prev1 = [torch.zeros(p.shape).to(device) for p in model.parameters()]\n",
        "prev2 = [torch.zeros(p.shape).to(device) for p in model.parameters()]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (samples1, samples2) in enumerate(zip(train_loader1, train_loader2)):\n",
        "\n",
        "        # Separate data per node\n",
        "        images1, labels1 = samples1[0].to(device), samples1[1].to(device)\n",
        "        images2, labels2 = samples2[0].to(device), samples2[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # First forward and backward pass\n",
        "        outs = model(images1)\n",
        "        loss1 = criterion(outs, labels1)\n",
        "        loss1.backward()\n",
        "        grad1, prev1 = map_gradients(model, num_bits, stochastic, prev1)\n",
        "\n",
        "        # Clear gradients from first pass (not necessary)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Second forward and backward pass\n",
        "        outs = model(images2)\n",
        "        loss2 = criterion(outs, labels2)\n",
        "        loss2.backward()\n",
        "        _, prev2 = map_gradients(model, num_bits, stochastic, prev2)\n",
        "\n",
        "        # Average the quantized gradients\n",
        "        for p, g in zip(model.parameters(), grad1):\n",
        "          p.grad.data.copy_(torch.add(p.grad.data, g))\n",
        "          p.grad.data.mul_(1.0 / nodes)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss2.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
        "\n",
        "        if epoch > 8:\n",
        "          mem_models += [(correct / total, copy.deepcopy(model), epoch+1)]\n",
        "\n",
        "test_and_save_best(mem_models, num_bits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVMH-MNktHiX",
        "outputId": "360aab90-ba2d-4042-cce2-1d96c30aca1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [352/352], Loss: 0.8602\n",
            "Accuracy of the network on the 5000 validation images: 57.12 %\n",
            "Epoch [2/30], Step [352/352], Loss: 0.7568\n",
            "Accuracy of the network on the 5000 validation images: 67.46 %\n",
            "Epoch [3/30], Step [352/352], Loss: 0.9965\n",
            "Accuracy of the network on the 5000 validation images: 72.76 %\n",
            "Epoch [4/30], Step [352/352], Loss: 0.5374\n",
            "Accuracy of the network on the 5000 validation images: 75.24 %\n",
            "Epoch [5/30], Step [352/352], Loss: 0.6035\n",
            "Accuracy of the network on the 5000 validation images: 74.48 %\n",
            "Epoch [6/30], Step [352/352], Loss: 0.5272\n",
            "Accuracy of the network on the 5000 validation images: 78.2 %\n",
            "Epoch [7/30], Step [352/352], Loss: 0.6479\n",
            "Accuracy of the network on the 5000 validation images: 78.32 %\n",
            "Epoch [8/30], Step [352/352], Loss: 0.4776\n",
            "Accuracy of the network on the 5000 validation images: 80.12 %\n",
            "Epoch [9/30], Step [352/352], Loss: 0.5016\n",
            "Accuracy of the network on the 5000 validation images: 79.42 %\n",
            "Epoch [10/30], Step [352/352], Loss: 0.6351\n",
            "Accuracy of the network on the 5000 validation images: 79.96 %\n",
            "Epoch [11/30], Step [352/352], Loss: 0.3990\n",
            "Accuracy of the network on the 5000 validation images: 80.5 %\n",
            "Epoch [12/30], Step [352/352], Loss: 0.5331\n",
            "Accuracy of the network on the 5000 validation images: 81.84 %\n",
            "Epoch [13/30], Step [352/352], Loss: 0.3479\n",
            "Accuracy of the network on the 5000 validation images: 81.64 %\n",
            "Epoch [14/30], Step [352/352], Loss: 0.2202\n",
            "Accuracy of the network on the 5000 validation images: 82.28 %\n",
            "Epoch [15/30], Step [352/352], Loss: 0.3332\n",
            "Accuracy of the network on the 5000 validation images: 83.06 %\n",
            "Epoch [16/30], Step [352/352], Loss: 0.3318\n",
            "Accuracy of the network on the 5000 validation images: 83.04 %\n",
            "Epoch [17/30], Step [352/352], Loss: 0.2775\n",
            "Accuracy of the network on the 5000 validation images: 83.54 %\n",
            "Epoch [18/30], Step [352/352], Loss: 0.2367\n",
            "Accuracy of the network on the 5000 validation images: 82.48 %\n",
            "Epoch [19/30], Step [352/352], Loss: 0.5035\n",
            "Accuracy of the network on the 5000 validation images: 83.84 %\n",
            "Epoch [20/30], Step [352/352], Loss: 0.2320\n",
            "Accuracy of the network on the 5000 validation images: 82.98 %\n",
            "Epoch [21/30], Step [352/352], Loss: 0.4024\n",
            "Accuracy of the network on the 5000 validation images: 83.48 %\n",
            "Epoch [22/30], Step [352/352], Loss: 0.5246\n",
            "Accuracy of the network on the 5000 validation images: 83.72 %\n",
            "Epoch [23/30], Step [352/352], Loss: 0.3953\n",
            "Accuracy of the network on the 5000 validation images: 83.76 %\n",
            "Epoch [24/30], Step [352/352], Loss: 0.2967\n",
            "Accuracy of the network on the 5000 validation images: 81.54 %\n",
            "Epoch [25/30], Step [352/352], Loss: 0.1461\n",
            "Accuracy of the network on the 5000 validation images: 83.08 %\n",
            "Epoch [26/30], Step [352/352], Loss: 0.3824\n",
            "Accuracy of the network on the 5000 validation images: 83.58 %\n",
            "Epoch [27/30], Step [352/352], Loss: 0.1126\n",
            "Accuracy of the network on the 5000 validation images: 83.44 %\n",
            "Epoch [28/30], Step [352/352], Loss: 0.2926\n",
            "Accuracy of the network on the 5000 validation images: 83.8 %\n",
            "Epoch [29/30], Step [352/352], Loss: 0.3373\n",
            "Accuracy of the network on the 5000 validation images: 83.88 %\n",
            "Epoch [30/30], Step [352/352], Loss: 0.3608\n",
            "Accuracy of the network on the 5000 validation images: 83.26 %\n",
            "Epoch : 29, Validation Accuracy : 0.8388\n",
            "Epoch : 19, Validation Accuracy : 0.8384\n",
            "Epoch : 28, Validation Accuracy : 0.838\n",
            "Accuracy of the network on the 10000 test images: 83.95 %\n",
            "Accuracy of the network on the 10000 test images: 83.84 %\n",
            "Accuracy of the network on the 10000 test images: 84.3 %\n",
            "Best model saved with accuracy: 84.3 %, at epoch : 28\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}